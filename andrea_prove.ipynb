{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e79010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\OneDrive\\Desktop\\UNI\\magistrale\\Reinforcement\\multi_agent\\env\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from ultimatetictactoe import ultimatetictactoe\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73d0449",
   "metadata": {},
   "outputs": [],
   "source": [
    "Policy = nn.Sequential(\n",
    "    nn.Conv2d(3, 2, 3, padding=1),\n",
    "    nn.Conv2d(2, 5, 3, padding=1),\n",
    "    nn.BatchNorm2d(5),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(5, 5, 3, padding=1),\n",
    "    nn.BatchNorm2d(5),\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(405, 405),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(405, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 81),\n",
    "    nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe6017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ultimatetictactoe.env(render_mode=\"human\")\n",
    "env.reset(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f2d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# for agent in env.agent_iter():\n",
    "#     print(agent)\n",
    "#     c += 1\n",
    "#     observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "#     if termination or truncation:\n",
    "#         action = None\n",
    "#     else:\n",
    "#         # this is where you would insert your policy\n",
    "#         mask = observation[\"action_mask\"]\n",
    "#         action = env.action_space(agent).sample(mask)\n",
    "\n",
    "#     env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27f47e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trajectory(env, agent1, agent2):\n",
    "    env.reset()\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    observation = env.last()[0]\n",
    "\n",
    "    counter = 0\n",
    "    for agent in env.agent_iter():\n",
    "        \n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        if termination or truncation:\n",
    "            break\n",
    "        state = observation['observation']\n",
    "\n",
    "        if agent == \"player_1\":\n",
    "            action = agent1.pick_action(env)\n",
    "        else:\n",
    "            action = agent2.pick_action(env)\n",
    "        \n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # rewards\n",
    "        if counter >= 2:\n",
    "            if agent == \"player_1\":\n",
    "                rewards\n",
    "        \n",
    "        env.step(action)\n",
    "        counter += 1\n",
    "    return states, actions, rewards\n",
    "\n",
    "def split_trajectory(trajectory):\n",
    "    # states, actions, rewards = trajectory\n",
    "    # return {\"player_1\": {'states': states[0::2], 'actions': actions[0::2], 'rewards': rewards}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08db957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, name, type='random'):\n",
    "        self.name = name\n",
    "    \n",
    "    def pick_action(self, env):\n",
    "        action_mask = env.last()[0]['action_mask']\n",
    "        return env.action_space(self.name).sample(action_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "794473f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e897887",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = Agent(\"player_1\")\n",
    "a2 = Agent(\"player_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b432fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.metadata['render_fps'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5915cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = trajectory(env, a1, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "92c1b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ce796a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 9])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(states[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(Policy.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f13abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePolicy(nn.Module):\n",
    "\n",
    "    def __init__(self, net = None):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        if net is None:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Conv2d(3, 3, 3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(243, 81),\n",
    "            )\n",
    "        else:\n",
    "            self.net = net\n",
    "\n",
    "        self.softmax = nn.Softmax(1)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor):\n",
    "        \"\"\"state should be tensor of shape (B, 3, 9, 9)\"\"\"\n",
    "        action_mask = state[:, 2, :, :].reshape(state.size(0), -1).bool()\n",
    "        logits = self.net(state) # (B, 81)\n",
    "        masked_logits = logits.masked_fill(~action_mask, float('-inf'))\n",
    "        probs = self.softmax(masked_logits)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf17fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAgent:\n",
    "    \"\"\"TODO\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        name, \n",
    "        policy_net, \n",
    "        optimizer,\n",
    "        device,\n",
    "        mode = 'train'\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.policy_net = policy_net\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.mode = mode\n",
    "\n",
    "    def pick_action(self, obs: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Obs is a (B, 3, 9, 9) tensor, where \n",
    "        the first two channels are the players moves on the board\n",
    "        and the last one is the action mask for the board\n",
    "        \"\"\"\n",
    "\n",
    "        probs = self.policy_net(obs.to(self.device))\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, log_prob\n",
    "\n",
    "    def update(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "d614fdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Categorical(probs: torch.Size([2, 81]))"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(0, 2, (2, 3, 9, 9)).to(torch.float)\n",
    "t = torch.rand((2, 81))\n",
    "torch.distributions.Categorical(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "34997cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "ag = NeuralAgent(\"ciao\", policy, optimizer, torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "f118fe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([44, 62]), tensor([-3.6152, -3.6094], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag.pick_action(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "dcf7e88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 7, 48]), tensor([-4.4060, -4.3525], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag.pick_action(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reinforce:\n",
    "\n",
    "    def __init__(self, env, num_episodes, agent1, agent2):\n",
    "        self.env = env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557a716d",
   "metadata": {},
   "source": [
    "# stable baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b74e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "import pettingzoo.utils\n",
    "from pettingzoo.classic import connect_four_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028a1c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on connect_four_v3.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 353      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20.7        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 304         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010001426 |\n",
      "|    clip_fraction        | 0.0792      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | -1.46       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0195     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.019      |\n",
      "|    value_loss           | 0.0843      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20          |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 302         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008765952 |\n",
      "|    clip_fraction        | 0.0494      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.9        |\n",
      "|    explained_variance   | -0.000405   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0264     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    value_loss           | 0.015       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 20.4        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 300         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009359449 |\n",
      "|    clip_fraction        | 0.0675      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | -0.791      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0213     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 0.00615     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 18.8        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 301         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010564503 |\n",
      "|    clip_fraction        | 0.0891      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | -0.712      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0268     |\n",
      "|    value_loss           | 0.00351     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 17          |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 301         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010750132 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | -0.552      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0398     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 0.00302     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 17.6        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 301         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009970924 |\n",
      "|    clip_fraction        | 0.0846      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.84       |\n",
      "|    explained_variance   | -0.404      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0217     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    value_loss           | 0.00242     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.7        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 300         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011427156 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.78       |\n",
      "|    explained_variance   | -0.0464     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0307     |\n",
      "|    value_loss           | 0.00194     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.1        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012378395 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.75       |\n",
      "|    explained_variance   | 0.0153      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0723     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    value_loss           | 0.00173     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 13.3        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 299         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012087636 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.073      |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0318     |\n",
      "|    value_loss           | 0.00143     |\n",
      "-----------------------------------------\n",
      "Model has been saved.\n",
      "Finished training on connect_four_v3.\n",
      "\n",
      "Starting evaluation vs a random agent. Trained agent will play as player_1.\n",
      "Rewards by round:  [{'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}, {'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}, {'player_0': 1, 'player_1': -1}]\n",
      "Total rewards (incl. negative rewards):  {'player_0': -60, 'player_1': 60}\n",
      "Winrate:  0.8\n",
      "Final scores:  {'player_0': 20, 'player_1': 80}\n",
      "Starting evaluation vs a random agent. Trained agent will play as player_1.\n",
      "Rewards by round:  [{'player_0': -1, 'player_1': 1}, {'player_0': -1, 'player_1': 1}]\n",
      "Total rewards (incl. negative rewards):  {'player_0': -2, 'player_1': 2}\n",
      "Winrate:  1.0\n",
      "Final scores:  {'player_0': 0, 'player_1': 2}\n"
     ]
    }
   ],
   "source": [
    "# To pass into other gymnasium wrappers, we need to ensure that pettingzoo's wrappper\n",
    "# can also be a gymnasium Env. Thus, we subclass under gym.Env as well.\n",
    "class SB3ActionMaskWrapper(pettingzoo.utils.BaseWrapper, gym.Env):\n",
    "    \"\"\"Wrapper to allow PettingZoo environments to be used with SB3 illegal action masking.\"\"\"\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Gymnasium-like reset function which assigns obs/action spaces to be the same for each agent.\n",
    "\n",
    "        This is required as SB3 is designed for single-agent RL and doesn't expect obs/action spaces to be functions\n",
    "        \"\"\"\n",
    "        super().reset(seed, options)\n",
    "\n",
    "        # Strip the action mask out from the observation space\n",
    "        self.observation_space = super().observation_space(self.possible_agents[0])[\n",
    "            \"observation\"\n",
    "        ]\n",
    "        self.action_space = super().action_space(self.possible_agents[0])\n",
    "\n",
    "        # Return initial observation, info (PettingZoo AEC envs do not by default)\n",
    "        return self.observe(self.agent_selection), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Gymnasium-like step function, returning observation, reward, termination, truncation, info.\n",
    "\n",
    "        The observation is for the next agent (used to determine the next action), while the remaining\n",
    "        items are for the agent that just acted (used to understand what just happened).\n",
    "        \"\"\"\n",
    "        current_agent = self.agent_selection\n",
    "\n",
    "        super().step(action)\n",
    "\n",
    "        next_agent = self.agent_selection\n",
    "        return (\n",
    "            self.observe(next_agent),\n",
    "            self._cumulative_rewards[current_agent],\n",
    "            self.terminations[current_agent],\n",
    "            self.truncations[current_agent],\n",
    "            self.infos[current_agent],\n",
    "        )\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"Return only raw observation, removing action mask.\"\"\"\n",
    "        return super().observe(agent)[\"observation\"]\n",
    "\n",
    "    def action_mask(self):\n",
    "        \"\"\"Separate function used in order to access the action mask.\"\"\"\n",
    "        return super().observe(self.agent_selection)[\"action_mask\"]\n",
    "\n",
    "\n",
    "def mask_fn(env):\n",
    "    # Do whatever you'd like in this function to return the action mask\n",
    "    # for the current env. In this example, we assume the env has a\n",
    "    # helpful method we can rely on.\n",
    "    return env.action_mask()\n",
    "\n",
    "\n",
    "def train_action_mask(env_fn, steps=10_000, seed=0, **env_kwargs):\n",
    "    \"\"\"Train a single model to play as each agent in a zero-sum game environment using invalid action masking.\"\"\"\n",
    "    env = env_fn.env(**env_kwargs)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "\n",
    "    # Custom wrapper to convert PettingZoo envs to work with SB3 action masking\n",
    "    env = SB3ActionMaskWrapper(env)\n",
    "\n",
    "    env.reset(seed=seed)  # Must call reset() in order to re-define the spaces\n",
    "\n",
    "    env = ActionMasker(env, mask_fn)  # Wrap to enable masking (SB3 function)\n",
    "    # MaskablePPO behaves the same as SB3's PPO unless the env is wrapped\n",
    "    # with ActionMasker. If the wrapper is detected, the masks are automatically\n",
    "    # retrieved and used when learning. Note that MaskablePPO does not accept\n",
    "    # a new action_mask_fn kwarg, as it did in an earlier draft.\n",
    "    model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=1)\n",
    "    model.set_random_seed(seed)\n",
    "    model.learn(total_timesteps=steps)\n",
    "\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\\n\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def eval_action_mask(env_fn, num_games=100, render_mode=None, **env_kwargs):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "    print(\n",
    "        f\"Starting evaluation vs a random agent. Trained agent will play as {env.possible_agents[1]}.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "\n",
    "    model = MaskablePPO.load(latest_policy)\n",
    "\n",
    "    scores = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    round_rewards = []\n",
    "\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "        env.action_space(env.possible_agents[0]).seed(i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            # Separate observation and action mask\n",
    "            observation, action_mask = obs.values()\n",
    "\n",
    "            if termination or truncation:\n",
    "                # If there is a winner, keep track, otherwise don't change the scores (tie)\n",
    "                if (\n",
    "                    env.rewards[env.possible_agents[0]]\n",
    "                    != env.rewards[env.possible_agents[1]]\n",
    "                ):\n",
    "                    winner = max(env.rewards, key=env.rewards.get)\n",
    "                    scores[winner] += env.rewards[\n",
    "                        winner\n",
    "                    ]  # only tracks the largest reward (winner of game)\n",
    "                # Also track negative and positive rewards (penalizes illegal moves)\n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                # List of rewards by round, for reference\n",
    "                round_rewards.append(env.rewards)\n",
    "                break\n",
    "            else:\n",
    "                if agent == env.possible_agents[0]:\n",
    "                    act = env.action_space(agent).sample(action_mask)\n",
    "                else:\n",
    "                    # Note: PettingZoo expects integer actions # TODO: change chess to cast actions to type int?\n",
    "                    act = int(\n",
    "                        model.predict(\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    # Avoid dividing by zero\n",
    "    if sum(scores.values()) == 0:\n",
    "        winrate = 0\n",
    "    else:\n",
    "        winrate = scores[env.possible_agents[1]] / sum(scores.values())\n",
    "    print(\"Rewards by round: \", round_rewards)\n",
    "    print(\"Total rewards (incl. negative rewards): \", total_rewards)\n",
    "    print(\"Winrate: \", winrate)\n",
    "    print(\"Final scores: \", scores)\n",
    "    return round_rewards, total_rewards, winrate, scores\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_fn = connect_four_v3\n",
    "\n",
    "    env_kwargs = {}\n",
    "\n",
    "    # Evaluation/training hyperparameter notes:\n",
    "    # 10k steps: Winrate:  0.76, loss order of 1e-03\n",
    "    # 20k steps: Winrate:  0.86, loss order of 1e-04\n",
    "    # 40k steps: Winrate:  0.86, loss order of 7e-06\n",
    "\n",
    "    # Train a model against itself (takes ~20 seconds on a laptop CPU)\n",
    "    train_action_mask(env_fn, steps=20_480, seed=0, **env_kwargs)\n",
    "\n",
    "    # Evaluate 100 games against a random agent (winrate should be ~80%)\n",
    "    eval_action_mask(env_fn, num_games=100, render_mode=None, **env_kwargs)\n",
    "\n",
    "    # Watch two games vs a random agent\n",
    "    eval_action_mask(env_fn, num_games=2, render_mode=\"human\", **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04b10d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
