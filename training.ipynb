{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298face",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\OneDrive\\Desktop\\UNI\\magistrale\\Reinforcement\\multi_agent\\env\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Uses Stable-Baselines3 to train agents in the Knights-Archers-Zombies environment using SuperSuit vector envs.\n",
    "\n",
    "This environment requires using SuperSuit's Black Death wrapper, to handle agent death.\n",
    "\n",
    "For more information, see https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "\n",
    "Author: Elliot (https://github.com/elliottower)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy, MlpPolicy\n",
    "\n",
    "# from pettingzoo.butterfly import knights_archers_zombies_v10\n",
    "from multi_agent.SuperTicTacToe.supertictactoe import ma_supertictactoe\n",
    "\n",
    "\n",
    "def train(env_fn, steps: int = 10_000, seed: int | None = 0, **env_kwargs):\n",
    "    # Train a single model to play as each agent in an AEC environment\n",
    "    env = env_fn.env()\n",
    "\n",
    "    # Add black death wrapper so the number of agents stays constant\n",
    "    # MarkovVectorEnv does not support environments with varying numbers of active agents unless black_death is set to True\n",
    "    env = ss.black_death_v3(env)\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    env = ss.concat_vec_envs_v1(env, 8, num_cpus=1, base_class=\"stable_baselines3\")\n",
    "\n",
    "    # Use a CNN policy if the observation space is visual\n",
    "    model = PPO(\n",
    "        CnnPolicy if visual_observation else MlpPolicy,\n",
    "        env,\n",
    "        verbose=3,\n",
    "        batch_size=256,\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=steps)\n",
    "\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def eval(env_fn, num_games: int = 100, render_mode: str | None = None, **env_kwargs):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn.env(render_mode=render_mode)\n",
    "\n",
    "    # Pre-process using SuperSuit\n",
    "    visual_observation = not env.unwrapped.vector_state\n",
    "    if visual_observation:\n",
    "        # If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\n",
    "        env = ss.color_reduction_v0(env, mode=\"B\")\n",
    "        env = ss.resize_v1(env, x_size=84, y_size=84)\n",
    "        env = ss.frame_stack_v1(env, 3)\n",
    "\n",
    "    print(\n",
    "        f\"\\nStarting evaluation on {str(env.metadata['name'])} (num_games={num_games}, render_mode={render_mode})\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "\n",
    "    model = PPO.load(latest_policy)\n",
    "\n",
    "    rewards = {agent: 0 for agent in env.possible_agents}\n",
    "\n",
    "    # Note: we evaluate here using an AEC environments, to allow for easy A/B testing against random policies\n",
    "    # For example, we can see here that using a random agent for archer_0 results in less points than the trained agent\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "        env.action_space(env.possible_agents[0]).seed(i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            for a in env.agents:\n",
    "                rewards[a] += env.rewards[a]\n",
    "\n",
    "            if termination or truncation:\n",
    "                break\n",
    "            else:\n",
    "                if agent == env.possible_agents[0]:\n",
    "                    act = env.action_space(agent).sample()\n",
    "                else:\n",
    "                    act = model.predict(obs, deterministic=True)[0]\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    avg_reward = sum(rewards.values()) / len(rewards.values())\n",
    "    avg_reward_per_agent = {\n",
    "        agent: rewards[agent] / num_games for agent in env.possible_agents\n",
    "    }\n",
    "    print(f\"Avg reward: {avg_reward}\")\n",
    "    print(\"Avg reward per agent, per game: \", avg_reward_per_agent)\n",
    "    print(\"Full rewards: \", rewards)\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b5570c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'raw_env' object has no attribute 'vector_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m env_fn = supertictactoe\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Set vector_state to false in order to use visual observations (significantly longer training time)\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Train a model (takes ~5 minutes on a laptop CPU)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m81_920\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Evaluate 10 games (takes ~10 seconds on a laptop CPU)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28meval\u001b[39m(env_fn, num_games=\u001b[32m10\u001b[39m, render_mode=\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(env_fn, steps, seed, **env_kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m env = ss.black_death_v3(env)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Pre-process using SuperSuit\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m visual_observation = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43munwrapped\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_state\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visual_observation:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# If the observation space is visual, reduce the color channels, resize from 512px to 84px, and apply frame stacking\u001b[39;00m\n\u001b[32m     35\u001b[39m     env = ss.color_reduction_v0(env, mode=\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'raw_env' object has no attribute 'vector_state'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env_fn = ma_supertictactoe\n",
    "    # Set vector_state to false in order to use visual observations (significantly longer training time)\n",
    "\n",
    "    # Train a model (takes ~5 minutes on a laptop CPU)\n",
    "    train(env_fn, steps=81_920, seed=0)\n",
    "\n",
    "    # Evaluate 10 games (takes ~10 seconds on a laptop CPU)\n",
    "    eval(env_fn, num_games=10, render_mode=None)\n",
    "\n",
    "    # Watch 2 games (takes ~10 seconds on a laptop CPU)\n",
    "    eval(env_fn, num_games=2, render_mode=\"human\")\n",
    "\n",
    "    # Set vector_state to false in order to use visual observations (significantly longer training time)\n",
    "    env_kwargs = dict(max_cycles=100, max_zombies=4, vector_state=True)\n",
    "\n",
    "    # Train a model (takes ~5 minutes on a laptop CPU)\n",
    "    train(env_fn, steps=81_920, seed=0)\n",
    "\n",
    "    # Evaluate 10 games (takes ~10 seconds on a laptop CPU)\n",
    "    eval(env_fn, num_games=10, render_mode=None)\n",
    "\n",
    "    # Watch 2 games (takes ~10 seconds on a laptop CPU)\n",
    "    eval(env_fn, num_games=2, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c5ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
